from typing import Any

from agentwerkstatt.llms.base import BaseLLM
from agentwerkstatt.tools.base import BaseTool


class ReflectionTool(BaseTool):
    """A tool to check if the generated final answer matches the initial request."""

    def __init__(self, llm_client: BaseLLM):
        self._llm_client = llm_client

    def get_name(self) -> str:
        """Returns the programmatic name of the tool."""
        return "reflection"

    def get_description(self) -> str:
        """Returns a human-readable description of what the tool does."""
        return """
            Checks if the generated final answer matches the initial request.
            Use it to verify that the final answer is relevant and complete.
            Use it as the last step in your execution process.
            """.strip()

    def get_schema(self) -> dict[str, Any]:
        """Returns the JSON schema for the tool's inputs."""
        return {
            "type": "function",
            "function": {
                "name": self.get_name(),
                "description": self.get_description(),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "initial_request": {
                            "type": "string",
                            "description": "The initial request from the user.",
                        },
                        "final_answer": {
                            "type": "string",
                            "description": "The final answer generated by the agent.",
                        },
                    },
                    "required": ["initial_request", "final_answer"],
                },
            },
        }

    def execute(self, **kwargs: Any) -> dict[str, Any]:
        """Executes the tool with the given keyword arguments."""
        initial_request = kwargs.get("initial_request")
        final_answer = kwargs.get("final_answer")
        if not initial_request or not final_answer:
            return {"error": "Initial request and final answer must be provided."}

        prompt = f"""
            Does the following final answer match the initial request?
            Initial Request: {initial_request}
            Final Answer: {final_answer}
            Answer with "yes" or "no" and a brief explanation.
            """.strip()
        try:
            reflection = self._llm_client.query(prompt=prompt, context="")
            return {"reflection": reflection}
        except Exception as e:
            return {"error": f"Failed to generate reflection: {e}"}
